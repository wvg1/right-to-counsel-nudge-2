import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import os
import json
import pickle
import time
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix

### load tensors and set up ###

checkpoint = torch.load('data/tensors.pt')
X_train_tensor = checkpoint['X_train']
X_val_tensor = checkpoint['X_val']
X_test_tensor = checkpoint['X_test']
y_train_tensors = checkpoint['y_train']
y_val_tensors = checkpoint['y_val']
y_test_tensors = checkpoint['y_test']
target_columns = checkpoint['target_columns']

print(f"tensors loaded from 'data/tensors.pt'\n")

### define nn architecture ###

class EvictionNet(nn.Module):
    """Neural network to predict eviction case outcomes."""
    
    def __init__(self, input_size=85, hidden_size_1=32, hidden_size_2=16):
        super(EvictionNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size_1)
        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)
        self.fc3 = nn.Linear(hidden_size_2, 1)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)
        x = self.sigmoid(x)
        return x

### create directories ###

os.makedirs("models", exist_ok=True)
os.makedirs("results", exist_ok=True)

### hyperparameter configuration ###

config = {
    'num_epochs': 100,
    'patience': 10,
    'batch_size': 32,
    'learning_rate': 0.001,
    'hidden_size_1': 32,
    'hidden_size_2': 16,
}

# save config
with open('results/training_config.json', 'w') as f:
    json.dump(config, f, indent=2)

print(f"training configuration saved to 'results/training_config.json'\n")

### instantiate models and optimizers ###

models = {col: EvictionNet(input_size=85, hidden_size_1=config['hidden_size_1'], 
                           hidden_size_2=config['hidden_size_2']) for col in target_columns}
optimizers = {col: torch.optim.Adam(models[col].parameters(), lr=config['learning_rate']) for col in target_columns}

print(f"models instantiated for {len(target_columns)} outcomes\n")

### create dataloaders ###

train_dataset = TensorDataset(
    X_train_tensor,
    *[y_train_tensors[col] for col in target_columns]
)
train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)

val_dataset = TensorDataset(
    X_val_tensor,
    *[y_val_tensors[col] for col in target_columns]
)
val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)

test_dataset = TensorDataset(
    X_test_tensor,
    *[y_test_tensors[col] for col in target_columns]
)
test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)

print(f"dataloaders created: {len(train_loader)} train, {len(val_loader)} val, {len(test_loader)} test\n")

### training configuration ###

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"using device: {device}\n")

# move models to device
for col in target_columns:
    models[col] = models[col].to(device)

### training function ###

def train_epoch(model, optimizer, train_loader, criterion, device, target_idx):
    """Train for one epoch and return average loss."""
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    for batch_data in train_loader:
        X_batch = batch_data[0].to(device)
        y_batch = batch_data[target_idx + 1].to(device)
        
        # check for NaN
        if torch.isnan(X_batch).any() or torch.isnan(y_batch).any():
            print("Warning: NaN detected in batch data")
            continue
        
        # forward pass
        predictions = model(X_batch)
        loss = criterion(predictions, y_batch)
        
        # check for NaN loss
        if np.isnan(loss.item()):
            print("Warning: NaN loss detected")
            continue
        
        # backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        num_batches += 1
    
    return total_loss / num_batches if num_batches > 0 else float('inf')

### validation function ###

def validate(model, val_loader, criterion, device, target_idx):
    """Evaluate model on validation set and return average loss."""
    model.eval()
    total_loss = 0.0
    num_batches = 0
    
    with torch.no_grad():
        for batch_data in val_loader:
            X_batch = batch_data[0].to(device)
            y_batch = batch_data[target_idx + 1].to(device)
            
            predictions = model(X_batch)
            loss = criterion(predictions, y_batch)
            
            total_loss += loss.item()
            num_batches += 1
    
    return total_loss / num_batches

### train all models ###

criterion = nn.BCELoss()
training_history = {col: {'train_loss': [], 'val_loss': [], 'epochs': 0, 'training_time': 0} for col in target_columns}
model_info = {}

for target_idx, target_col in enumerate(target_columns):
    model = models[target_col]
    optimizer = optimizers[target_col]
    
    print(f"\n{'='*60}")
    print(f"training model: {target_col}")
    print(f"{'='*60}\n")
    
    best_val_loss = float('inf')
    patience_counter = 0
    best_epoch = 0
    start_time = time.time()
    
    for epoch in range(config['num_epochs']):
        # train and validate
        train_loss = train_epoch(model, optimizer, train_loader, criterion, device, target_idx)
        val_loss = validate(model, val_loader, criterion, device, target_idx)
        
        training_history[target_col]['train_loss'].append(train_loss)
        training_history[target_col]['val_loss'].append(val_loss)
        
        # print progress every 10 epochs
        if (epoch + 1) % 10 == 0:
            print(f"epoch {epoch+1:3d}  |  train_loss: {train_loss:.4f}  |  val_loss: {val_loss:.4f}")
        
        # early stopping check
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            best_epoch = epoch + 1
            # save best model
            torch.save(model.state_dict(), f"models/{target_col}_best.pt")
            # save checkpoint
            checkpoint = {
                'epoch': epoch,
                'model_state': model.state_dict(),
                'optimizer_state': optimizer.state_dict(),
                'best_val_loss': best_val_loss,
            }
            torch.save(checkpoint, f"models/{target_col}_checkpoint.pt")
        else:
            patience_counter += 1
            if patience_counter >= config['patience']:
                print(f"\nearly stopping at epoch {epoch+1} (best val_loss: {best_val_loss:.4f})")
                break
    
    elapsed_time = time.time() - start_time
    training_history[target_col]['epochs'] = best_epoch
    training_history[target_col]['training_time'] = elapsed_time
    
    model_info[target_col] = {
        'best_epoch': best_epoch,
        'best_val_loss': best_val_loss,
        'training_time_seconds': elapsed_time,
        'final_train_loss': training_history[target_col]['train_loss'][-1],
    }
    
    print(f"training complete. best val_loss: {best_val_loss:.4f} at epoch {best_epoch}")
    print(f"training time: {elapsed_time:.2f} seconds\n")

### save training history ###

with open('results/training_history.pkl', 'wb') as f:
    pickle.dump(training_history, f)

with open('results/model_info.json', 'w') as f:
    json.dump(model_info, f, indent=2)

print(f"training history saved to 'results/training_history.pkl'\n")

### plot training curves ###

print("generating loss curves...")

for col in target_columns:
    plt.figure(figsize=(10, 6))
    plt.plot(training_history[col]['train_loss'], label='train loss', linewidth=2)
    plt.plot(training_history[col]['val_loss'], label='val loss', linewidth=2)
    plt.xlabel('epoch', fontsize=12)
    plt.ylabel('loss (BCE)', fontsize=12)
    plt.title(f'{col} - Training History', fontsize=14)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'results/{col}_loss_curve.png', dpi=150)
    plt.close()

print(f"loss curves saved to 'results/'\n")

### evaluate on test set ###

def test_model(model, test_loader, criterion, device, target_idx):
    """Evaluate model on test set and return loss and predictions."""
    model.eval()
    total_loss = 0.0
    num_batches = 0
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch_data in test_loader:
            X_batch = batch_data[0].to(device)
            y_batch = batch_data[target_idx + 1].to(device)
            
            predictions = model(X_batch)
            loss = criterion(predictions, y_batch)
            
            total_loss += loss.item()
            num_batches += 1
            
            all_predictions.append(predictions.cpu().numpy())
            all_targets.append(y_batch.cpu().numpy())
    
    avg_loss = total_loss / num_batches
    predictions = np.concatenate(all_predictions, axis=0)
    targets = np.concatenate(all_targets, axis=0)
    
    return avg_loss, predictions, targets

print(f"\n{'='*60}")
print("evaluating on test set")
print(f"{'='*60}\n")

test_results = {}
performance_metrics = {}

for target_idx, target_col in enumerate(target_columns):
    model = models[target_col]
    
    # load best model
    model.load_state_dict(torch.load(f"models/{target_col}_best.pt"))
    
    test_loss, predictions, targets = test_model(model, test_loader, criterion, device, target_idx)
    test_results[target_col] = {
        'test_loss': test_loss,
        'predictions': predictions,
        'targets': targets
    }
    
    # calculate metrics
    auc = roc_auc_score(targets, predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(targets, predictions > 0.5, average='binary')
    tn, fp, fn, tp = confusion_matrix(targets, predictions > 0.5).ravel()
    specificity = tn / (tn + fp)
    
    performance_metrics[target_col] = {
        'test_loss': test_loss,
        'auc': auc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'specificity': specificity,
        'true_positives': int(tp),
        'true_negatives': int(tn),
        'false_positives': int(fp),
        'false_negatives': int(fn),
    }
    
    print(f"{target_col:30s}")
    print(f"  test_loss: {test_loss:.4f}  |  AUC: {auc:.4f}  |  F1: {f1:.4f}")
    print(f"  precision: {precision:.4f}  |  recall: {recall:.4f}  |  specificity: {specificity:.4f}\n")

### save performance metrics ###

with open('results/performance_metrics.json', 'w') as f:
    json.dump(performance_metrics, f, indent=2)

print(f"performance metrics saved to 'results/performance_metrics.json'\n")

print(f"{'='*60}")
print("training and evaluation complete!")
print(f"{'='*60}")